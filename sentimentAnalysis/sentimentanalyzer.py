# -*- coding: utf-8 -*-
"""sentimentAnalyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KE732ruaMhJ25JkfrMiCoOYWUdAAk8R
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import nltk

from nltk.stem import WordNetLemmatizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from bs4 import BeautifulSoup

import warnings
warnings.filterwarnings('ignore')

lemmatizer_ = WordNetLemmatizer()
stop_words = set(w.rstrip() for w in open("stopwords.txt"))

"""Use BeautifulSoup to read XML files"""

positive_reviews = BeautifulSoup(open('/content/positive.review').read())
positive_reviews = positive_reviews.findAll('review_text')

negative_reviews = BeautifulSoup(open('/content/negative.review').read())
negative_reviews = negative_reviews.findAll('review_text')

"""To make sure that the data is equal for both the classes, get only the `len(negative_reviews)` number of samples from positive_reviews."""

np.random.shuffle(positive_reviews)
positive_reviews = positive_reviews[:len(negative_reviews)]

word_index_map = {}
idx = 0

nltk.download('punkt')

nltk.download('wordnet')

"""Defining a tokenizer using nltk tokenizers and lemmatizer.

Lemmatizer âŸ¶ Generates root word for a given word.

Lemmatization helps in decrease of vocab size, by replacing the words with same meaning by its root word
"""

def tokenizer_with_nltk(s):
  s = s.lower()
  tokens = nltk.tokenize.word_tokenize(s)
  tokens = [tok for tok in tokens if len(tok)>2] # assuming anyword of 2 or less no. of letters is not useful or meaningful
  tokens = [lemmatizer_.lemmatize(t) for t in tokens]
  tokens = [t for t in tokens if t not in stop_words]
  return tokens

positive_tokenized = []
negative_tokenized = []

"""Get tokens and add to dictionary for both positive and negative reviews."""

for review in positive_reviews:
  tokens = tokenizer_with_nltk(review.text)
  positive_tokenized.append(tokens)
  for token in tokens:
    if token not in word_index_map:
      word_index_map[token] = idx
      idx += 1

for review in negative_reviews:
  tokens = tokenizer_with_nltk(review.text)
  negative_tokenized.append(tokens)
  for token in tokens:
    if token not in word_index_map:
      word_index_map[token] = idx
      idx += 1

word_index_map['life']

"""We now need X as a vector of counts of each word.

For eg: columns are "yes", "good", "no", "bad"

`x = [2, 1, 6, 8]`

here x says, that in the text of that particular input has `yes` 2 times, `bad` 8 times and etc

Defining a function that return data vector for a given `input.text` and `label`.
"""

def tokens_to_vector(tokens, label):
  x = np.zeros(len(word_index_map) + 1)
  for t in tokens:
    i = word_index_map[t]
    x[i] += 1 # increment the count of that word i.e index
  x = x/x.sum()
  x[-1] = label
  return x

X_positive = []
for each_input in positive_tokenized:
  data_input = tokens_to_vector(each_input, 1)
  X_positive.append(data_input)

X_negative = []
for each_input in negative_tokenized:
  data_input = tokens_to_vector(each_input, 0)
  X_negative.append(data_input)

df1 = pd.DataFrame(X_positive)
df2 = pd.DataFrame(X_negative)
df = pd.concat([df1, df2], axis=0)

X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:,-1:], shuffle = True, random_state = 42, train_size = 0.80)

model = LogisticRegression()
model.fit(X_train, y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

print("Classification rate: \n")
print("Train set: ", model.score(X_train, y_train))
print("Test set:  ", model.score(X_test, y_test))

print(f"The Confusion Matrix for training dataset: \n {confusion_matrix(y_train, y_train_pred)}")

print(f"The Confusion Matrix for training dataset: \n {confusion_matrix(y_test, y_test_pred)}")

threshold = 0.5

print("Positive weights --> imply words in Positive reviews\n")

for word, index in word_index_map.items():
  weight = model.coef_[0][index]
  if weight > threshold:
    print(f"The weight of : '{word}' is {weight}")

print("\n\nNegative weights --> imply words in Negative reviews\n")
for word, index in word_index_map.items():
  weight = model.coef_[0][index]
  if weight < -1*threshold:
    print(f"The weight of : '{word}' is {weight}")

